{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/arsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/arsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/arsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "import gspread\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/arsh/Desktop/intern1/JavaBasics-notes.pdf'\n",
    "# open allows you to open the files\n",
    "pdfobj = open(file,'rb')\n",
    "# pdfReader is a readable object that will be parsed.\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfobj)\n",
    "# discerning the number of pages will allow us to parse a number of pages.\n",
    "num_pages = pdfReader.numPages\n",
    "count=0\n",
    "text=''\n",
    "# the while loop will read each page\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count += 1\n",
    "    text += pageObj.extractText()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text into key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Java',\n",
       " 'BasicsÂ©',\n",
       " '1996-2003',\n",
       " 'jGuru.com',\n",
       " '.',\n",
       " 'All',\n",
       " 'Rights',\n",
       " 'Reserved.Java',\n",
       " 'Basics',\n",
       " '-1Java']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing on Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['java',\n",
       " 'right',\n",
       " 'basic',\n",
       " 'basicstopics',\n",
       " 'section',\n",
       " 'include',\n",
       " 'make',\n",
       " 'java',\n",
       " 'program',\n",
       " 'portable']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all words to lower case.\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "# remove all non-alphanumeric words\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "# remove all stopwords\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "# lemmatize all the words.\n",
    "wrdntlzr = WordNetLemmatizer()\n",
    "lemmatized = [ wrdntlzr.lemmatize(t) for t in no_stops ]\n",
    "lemmatized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('java', 71),\n",
       " ('new', 47),\n",
       " ('data', 42),\n",
       " ('applet', 37),\n",
       " ('object', 37),\n",
       " ('button', 36),\n",
       " ('array', 30),\n",
       " ('class', 29),\n",
       " ('int', 29),\n",
       " ('code', 27),\n",
       " ('method', 27),\n",
       " ('string', 26),\n",
       " ('b', 26),\n",
       " ('basic', 24),\n",
       " ('right', 23),\n",
       " ('public', 21),\n",
       " ('program', 18),\n",
       " ('example', 18),\n",
       " ('type', 15),\n",
       " ('language', 14)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_simple = Counter(lemmatized)\n",
    "bow_simple.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using tf-idf with Gensim to find most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reserved', 0.9402500069807611),\n",
       " ('comment', 0.7277421278035406),\n",
       " ('data', 0.6013084800711328),\n",
       " ('blank', 0.5),\n",
       " ('intentionally', 0.5),\n",
       " ('left', 0.5),\n",
       " ('page', 0.5),\n",
       " ('bit', 0.4511811739960831),\n",
       " ('label', 0.3925016345955494),\n",
       " ('ints', 0.3763637381615694)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a list of pages\n",
    "list_of_pages=[]\n",
    "count=0\n",
    "\n",
    "#extracting all pages and adding as a new item to the list_of_pages\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count += 1\n",
    "    list_of_pages += [ pageObj.extractText() ]\n",
    "    \n",
    "#lowercasing every word in all the pages\n",
    "lower_list_of_pages = [ page.lower() for page in list_of_pages]\n",
    "\n",
    "# word_tokenising every page in the list_of_pages and adding as a list to lop_tokenized\n",
    "lop_tokenized = [ word_tokenize(t) for t in lower_list_of_pages ]\n",
    "\n",
    "# initialising a lemmatized_pages_list that will contain the lists of tokenised words after preprocessing\n",
    "lemmatized_pages_list = []\n",
    "\n",
    "for t in lop_tokenized:\n",
    "    alpha_only = [p for p in t if p.isalpha()]\n",
    "    no_stops = [ p for p in alpha_only if p not in stopwords.words('english') ]\n",
    "    lemmatized = [ wrdntlzr.lemmatize(p) for p in no_stops ]\n",
    "    lemmatized_pages_list+=[lemmatized] \n",
    "    \n",
    "# initialising a dictionary object on lemmatized_pages_list\n",
    "dictionary = Dictionary(lemmatized_pages_list)\n",
    "\n",
    "# creating a bag-of-words corpus from dictionary for every page in lemmatized_pages_list\n",
    "corpus = [ dictionary.doc2bow(doc) for doc in lemmatized_pages_list]\n",
    "\n",
    "# instantiating an tfidf object\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# making a new dictionary that would contain all the important words along with their weights\n",
    "dic = {}\n",
    "for doc in corpus:\n",
    "    tfidf_weight = tfidf[doc]\n",
    "    sorted_tfidf_weights = sorted(tfidf_weight, key=lambda w: w[1], reverse=True)\n",
    "    # Print the top 5 weighted words\n",
    "    for term_id, weight in sorted_tfidf_weights:\n",
    "        dic[dictionary.get(term_id)]= weight\n",
    "\n",
    "# creating a list of tuples from the dictionary in descending order of their values.\n",
    "sorted_by_value = sorted(dic.items(), key=lambda kv: kv[1],reverse = True)\n",
    "sorted_by_value[:10]"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the dictionary to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reserved</td>\n",
       "      <td>0.940250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>0.727742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data</td>\n",
       "      <td>0.601308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blank</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>page</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>intentionally</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>left</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bit</td>\n",
       "      <td>0.451181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>label</td>\n",
       "      <td>0.392502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ints</td>\n",
       "      <td>0.376364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        keywords   weights\n",
       "0       reserved  0.940250\n",
       "1        comment  0.727742\n",
       "2           data  0.601308\n",
       "3          blank  0.500000\n",
       "4           page  0.500000\n",
       "5  intentionally  0.500000\n",
       "6           left  0.500000\n",
       "7            bit  0.451181\n",
       "8          label  0.392502\n",
       "9           ints  0.376364"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(dic.items()), columns=['keywords','weights'])\n",
    "df = df.sort_values('weights',ascending=False).reset_index()\n",
    "del df['index']\n",
    "df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the dataframe to an excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('keywordsAndWeights.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
